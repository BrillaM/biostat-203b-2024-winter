---
title: "Biostat 203B Homework 5"
subtitle: Due Mar 22 @ 11:59PM
author: "Brilla Meng, UID:806329681"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
---

## Predicting ICU duration

Using the ICU cohort `mimiciv_icu_cohort.rds` you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient's ICU stay will be longer than 2 days. You should use the `los_long` variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU `intime`, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay's `intime`. For instance, `last_careunit` cannot be used in your algorithms. 

1. Data preprocessing and feature engineering.

```{r}
library(GGally)
library(gtsummary)
library(dplyr)
library(tidymodels)
library(tidyverse)
library(ranger)
```

```{r}
mimic_icu_cohort <- readRDS("mimic_icu_cohort.rds")
```

```{r}
mimic_icu_cohort <- mimic_icu_cohort %>%
  select(-c(
    last_careunit,
    intime,
    outtime,
    los,
    deathtime,
    admittime,
    dischtime,
    discharge_location,
    dod,
    anchor_year_group,
    edregtime,
    edouttime,
    age_intime,
    admit_provider_id
  )) %>%
mutate(los_long = as.factor(los_long),
       gender = as.factor(gender),
       insurance = as.factor(insurance),
       language = as.factor(language),
       marital_status = as.factor(marital_status)
       )
```


2. Partition data into 50% training set and 50% test set. Stratify partitioning according to `los_long`. For grading purpose, sort the data by `subject_id`, `hadm_id`, and `stay_id` and use the seed `203` for the initial data split. Below is the sample code.
```{r}
set.seed(203)

# sort
mimiciv_icu_cohort <- mimic_icu_cohort |>
  arrange(subject_id, hadm_id, stay_id) |>
  select(-subject_id, -hadm_id, -stay_id)

data_split <- initial_split(
  mimiciv_icu_cohort,
  # stratify by los_long
  strata = "los_long",
  prop = 0.5
)
mimic_other <- training(data_split)
mimic_test <- testing(data_split)
dim(mimic_other)
dim(mimic_test)
```

3. Train and tune the models using the training set.

#Lasso
```{r}
norm_recipe <-
  recipe(
    los_long ~ .,
    data = mimic_other
  ) %>%
  # create traditional dummy variables
  step_impute_mean(all_numeric(),-all_outcomes()) %>%
  step_impute_mode(all_nominal(),-all_outcomes()) %>%
  step_dummy(all_nominal(),-all_outcomes()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())
  # center and scale numeric data
  # step_log(Salary, base = 10) %>%
  # estimate the means and standard deviations
norm_recipe
```

```{r}
logit_mod <-
  logistic_reg(penalty = tune(), mixture = tune()
               ) %>%
  set_engine("glmnet", standardize = FALSE) %>%
  print()
```

```{r}
logit_workflow <-
  workflow() %>%
  add_recipe(norm_recipe) %>%
  add_model(logit_mod)
logit_workflow
```

```{r}
param_grid <- grid_regular(
  penalty(
    range = c(-6, 3)),
  mixture(),
  levels = c(100,5)
) %>%
  print()
```

```{r}
set.seed(203)

folds <- vfold_cv(mimic_other, v = 5)
folds
```

```{r}
logit_fit <-
  logit_workflow %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
  )
logit_fit
```

```{r}
logit_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = penalty, y = mean)) + 
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") + 
  scale_x_log10()
```

```{r}
logit_fit %>%
  show_best("roc_auc")
```

```{r}
best_logit <-logit_fit %>%
  select_best("roc_auc")
best_logit
```

```{r}
final_wf <- logit_workflow %>%
  finalize_workflow(best_logit)
final_wf
```

```{r}
final_fit <-
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
final_fit %>%
  collect_metrics()
```

```{r}
final_tree <- extract_workflow(final_fit)
final_tree
```

```{r}
library(vip)

final_tree %>%
 extract_fit_parsnip() %>%
  vip()
```

# Random Forest
```{r}
rf_mod <-
  rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune()
  ) %>%
  set_engine("ranger")
```

```{r}
rf_workflow <-
  workflow() %>%
  add_recipe(norm_recipe) %>%
  add_model(rf_mod)
```

```{r}
param_grid <- grid_regular(
  trees(range = c(100L, 300L)),
  mtry(range = c(1L, 5L)),
  levels = c(5,5)
  )
param_grid
```


```{r}
rf_fit <-
  rf_workflow %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
  )
rf_fit
```

```{r}
rf_fit %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = trees, y = mean)) + 
  geom_point() + 
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV AUC")
```

```{r}
rf_fit %>%
  show_best("roc_auc")
```

```{r}
best_rf <- rf_fit %>%
  select_best("roc_auc")
best_rf
```

```{r}
final_wf <- rf_workflow %>%
  finalize_workflow(best_rf)
final_wf
```

```{r}
final_fit <-
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
final_fit %>%
  collect_metrics()
```



# XGBoost
```{r}
gb_mod <-
  boost_tree(
    mode = "classification",
    trees = 1000,
    tree_depth = tune(),
    learn_rate = tune()
  ) %>%
  set_engine("xgboost")
gb_mod
```

```{r}
gb_workflow <-
  workflow() %>%
  add_recipe(norm_recipe) %>%
  add_model(gb_mod)
gb_workflow
```

```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
  )
param_grid
```

```{r}
gb_fit <-
  gb_workflow %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
  )
gb_fit
```

```{r}
gb_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```

```{r}
gb_fit %>%
  show_best("roc_auc")
```

```{r}
best_gb <- gb_fit %>%
  select_best("roc_auc")
best_gb
```

```{r}
final_wf <- gb_workflow %>%
  finalize_workflow(best_gb)
final_wf
```

```{r}
final_fit <-
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
final_fit %>%
  collect_metrics()
```


4. Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each machine learning algorithm and the model stacking. Interpret the results. What are the most important features in predicting long ICU stays? How do the models compare in terms of performance and interpretability?

Lasso Logistic Regression: The best performing Lasso model, as per the images, had an ROC AUC of approximately 0.617 and accuracy of about 0.58. Lasso models are known to enforce sparsity, which can help in feature selection by shrinking the coefficients of less important variables to zero.
Random Forest: The Random Forest model's performance, as displayed in the images, shows an ROC AUC ranging from about 0.65 to 0.66 across different numbers of trees used. Random Forest is an ensemble method which can handle a large number of features and can capture complex structures in the data, often performing well out of the box.
XGBoost: The XGBoost model configurations showed results with an ROC AUC of approximately 0.658 at best, which is slightly better than the Random Forest models, and an accuracy of 0.611. XGBoost is another ensemble method that uses gradient boosting framework and is well-known for its performance in structured datasets.
The XGBoost has the highest ROC AUC, so it considered the best performer among the three models.However, the accuracy is comparable with the Lasso model. Even XGBoost and Random Forest have the best performance metrics, they are generally considered less interpretable than linear models like Lasso, due to their complex structure of multiple decision trees.
XGboost has the better performance than Random Forest, both of them are better than Lasso logistic Regression. Lasso Logistic Regression offers the most interpretable model due to its linear nature. The coefficients of the model can directly indicate the importance and the direction of the relationship of each feature with the outcome.There's a trade-off between the performance and interpretability of these models. While ensemble methods like Random Forest and XGBoost may offer better prediction performance, they do so at the expense of being less interpretable.
Importent Features:
Non Invasive Blood Pressure systolic,
first careunit Medical.lntensive.Care.Unit..MlCU.,
first careunit Medical.Surgical.intensive.Care.Unit..MiCU.SICU,
admission locatiOn TRANSFER.FROM.HOSPITAL,
Respiratory Rate,
hospital expire flag,
Temperature Fahrenheit,
admission tyPe SURGICAL.SAME.DAY.ADMISSION,
Hematocrit,
White Blood Cells.
Those are the most 10 important features in predicting long ICU stays. From the most importent which is around 2.0, to the less importent which is around 0.1.



